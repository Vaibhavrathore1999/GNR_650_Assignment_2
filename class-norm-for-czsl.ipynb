{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/universome/class-norm-for-czsl/blob/master/class-norm-for-czsl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ML0uxd0ft-ZV"
   },
   "source": [
    "#### 1. Defining the hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "t_Gl6w5ft-ZV"
   },
   "outputs": [],
   "source": [
    "DATASET = 'AWA2' # One of [\"AWA1\", \"AWA2\", \"APY\", \"CUB\", \"SUN\"]\n",
    "USE_CLASS_STANDARTIZATION = True # i.e. equation (9) from the paper\n",
    "USE_PROPER_INIT = True # i.e. equation (10) from the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLyeh2lit-ZW"
   },
   "source": [
    "#### 2. Downloading GBU data from [the official GBU website](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/zero-shot-learning/zero-shot-learning-the-good-the-bad-and-the-ugly) (takes 1-2 minutes for the first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6qt2FThst-ZW"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ -d \"./data\" ] \n",
    "then\n",
    "    echo \"Files are already there.\"\n",
    "else\n",
    "    wget -q \"http://datasets.d2.mpi-inf.mpg.de/xian/xlsa17.zip\"\n",
    "    unzip -q xlsa17.zip -d ./data\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XW6mEZNyt-ZX"
   },
   "source": [
    "#### 3. Running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jh0MKpUtt-ZX",
    "outputId": "9761732b-c765-43af-8fca-1ac71057f8c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<=============== Loading data for AWA2 ===============>\n",
      "<=============== Preprocessing ===============>\n",
      "\n",
      "<=============== Starting training ===============>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_161780/1612661871.py:132: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  feats = torch.from_numpy(np.array(batch[0])).to(DEVICE)\n",
      "/tmp/ipykernel_161780/1612661871.py:133: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  targets = torch.from_numpy(np.array(batch[1])).to(DEVICE)\n",
      "100%|██████████| 100/100 [01:03<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done! Took time:  63.3 seconds\n",
      "ZSL-U: 32.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DATASET = 'AWA2' # One of [\"AWA1\", \"AWA2\", \"APY\", \"CUB\", \"SUN\"]\n",
    "USE_CLASS_STANDARTIZATION = True # i.e. equation (9) from the paper\n",
    "USE_PROPER_INIT = True # i.e. equation (10) from the paper\n",
    "\n",
    "import numpy as np; np.random.seed(1)\n",
    "import torch; torch.manual_seed(1)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from scipy import io\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "print(f'<=============== Loading data for {DATASET} ===============>')\n",
    "DEVICE = 'cuda:1' # Set to 'cpu' if a GPU is not available\n",
    "DATA_DIR = f'./IAB-GZSL/data/{DATASET}'\n",
    "data = io.loadmat(f'{DATA_DIR}/vitL14.mat')\n",
    "attrs_mat = io.loadmat(f'{DATA_DIR}/att_splits.mat')\n",
    "feats = data['features'].T.astype(np.float32)\n",
    "labels = data['labels'].squeeze() - 1 # Using \"-1\" here and for idx to normalize to 0-index\n",
    "train_idx = attrs_mat['trainval_loc'].squeeze() - 1\n",
    "test_seen_idx = attrs_mat['test_seen_loc'].squeeze() - 1\n",
    "test_unseen_idx = attrs_mat['test_unseen_loc'].squeeze() - 1\n",
    "test_idx = np.array(test_seen_idx.tolist() + test_unseen_idx.tolist())\n",
    "seen_classes = sorted(np.unique(labels[test_seen_idx]))\n",
    "unseen_classes = sorted(np.unique(labels[test_unseen_idx]))\n",
    "\n",
    "\n",
    "print(f'<=============== Preprocessing ===============>')\n",
    "num_classes = len(seen_classes) + len(unseen_classes)\n",
    "seen_mask = np.array([(c in seen_classes) for c in range(num_classes)])\n",
    "unseen_mask = np.array([(c in unseen_classes) for c in range(num_classes)])\n",
    "attrs = attrs_mat['att'].T\n",
    "attrs = torch.from_numpy(attrs).to(DEVICE).float()\n",
    "attrs = attrs / attrs.norm(dim=1, keepdim=True) * np.sqrt(attrs.shape[1])\n",
    "attrs_seen = attrs[seen_mask]\n",
    "attrs_unseen = attrs[unseen_mask]\n",
    "train_labels = labels[train_idx]\n",
    "test_labels = labels[test_idx]\n",
    "test_seen_idx = [i for i, y in enumerate(test_labels) if y in seen_classes]\n",
    "test_unseen_idx = [i for i, y in enumerate(test_labels) if y in unseen_classes]\n",
    "labels_remapped_to_seen = [(seen_classes.index(t) if t in seen_classes else -1) for t in labels]\n",
    "test_labels_remapped_seen = [(seen_classes.index(t) if t in seen_classes else -1) for t in test_labels]\n",
    "test_labels_remapped_unseen = [(unseen_classes.index(t) if t in unseen_classes else -1) for t in test_labels]\n",
    "ds_train = [(feats[i], labels_remapped_to_seen[i]) for i in train_idx]\n",
    "ds_test = [(feats[i], int(labels[i])) for i in test_idx]\n",
    "train_dataloader = DataLoader(ds_train, batch_size=256, shuffle=True)\n",
    "test_dataloader = DataLoader(ds_test, batch_size=2048)\n",
    "\n",
    "class_indices_inside_test = {c: [i for i in range(len(test_idx)) if labels[test_idx[i]] == c] for c in range(num_classes)}\n",
    "\n",
    "\n",
    "class ClassStandardization(nn.Module):\n",
    "    \"\"\"\n",
    "    Class Standardization procedure from the paper.\n",
    "    Conceptually, it is equivalent to nn.BatchNorm1d with affine=False,\n",
    "    but for some reason nn.BatchNorm1d performs slightly worse.\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.running_mean = nn.Parameter(torch.zeros(feat_dim), requires_grad=False)\n",
    "        self.running_var = nn.Parameter(torch.ones(feat_dim), requires_grad=False)\n",
    "    \n",
    "    def forward(self, class_feats):\n",
    "        \"\"\"\n",
    "        Input: class_feats of shape [num_classes, feat_dim]\n",
    "        Output: class_feats (standardized) of shape [num_classes, feat_dim]\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            batch_mean = class_feats.mean(dim=0)\n",
    "            batch_var = class_feats.var(dim=0)\n",
    "            \n",
    "            # Normalizing the batch\n",
    "            result = (class_feats - batch_mean.unsqueeze(0)) / (batch_var.unsqueeze(0) + 1e-5)\n",
    "            \n",
    "            # Updating the running mean/std\n",
    "            self.running_mean.data = 0.9 * self.running_mean.data + 0.1 * batch_mean.detach()\n",
    "            self.running_var.data = 0.9 * self.running_var.data + 0.1 * batch_var.detach()\n",
    "        else:\n",
    "            # Using accumulated statistics\n",
    "            # Attention! For the test inference, we cant use batch-wise statistics,\n",
    "            # only the accumulated ones. Otherwise, it will be quite transductive\n",
    "            result = (class_feats - self.running_mean.unsqueeze(0)) / (self.running_var.unsqueeze(0) + 1e-5)\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "class CNZSLModel(nn.Module):\n",
    "    def __init__(self, attr_dim: int, hid_dim: int, proto_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(attr_dim, hid_dim),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(hid_dim, hid_dim),\n",
    "            ClassStandardization(hid_dim) if USE_CLASS_STANDARTIZATION else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            ClassStandardization(hid_dim) if USE_CLASS_STANDARTIZATION else nn.Identity(),\n",
    "            nn.Linear(hid_dim, proto_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        if USE_PROPER_INIT:\n",
    "            weight_var = 1 / (hid_dim * proto_dim)\n",
    "            b = np.sqrt(3 * weight_var)\n",
    "            self.model[-2].weight.data.uniform_(-b, b)\n",
    "        \n",
    "    def forward(self, x, attrs):\n",
    "        protos = self.model(attrs)\n",
    "        x_ns = 5 * x / x.norm(dim=1, keepdim=True) # [batch_size, x_dim]\n",
    "        protos_ns = 5 * protos / protos.norm(dim=1, keepdim=True) # [num_classes, x_dim]\n",
    "        logits = x_ns @ protos_ns.t() # [batch_size, num_classes]\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "\n",
    "print(f'\\n<=============== Starting training ===============>')\n",
    "start_time = time()\n",
    "model = CNZSLModel(attrs.shape[1],1024, feats.shape[1]).to(DEVICE)\n",
    "optim = torch.optim.Adam(model.model.parameters(), lr=0.0005, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optim, gamma=0.1, step_size=25)\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(100)):\n",
    "    model.train()\n",
    "    \n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        feats = torch.from_numpy(np.array(batch[0])).to(DEVICE)\n",
    "        targets = torch.from_numpy(np.array(batch[1])).to(DEVICE)\n",
    "        logits = model(feats, attrs[seen_mask])\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "print(f'Training is done! Took time: {(time() - start_time): .1f} seconds')\n",
    "\n",
    "model.eval() # Important! Otherwise we would use unseen batch statistics\n",
    "logits = [model(x.to(DEVICE), attrs).cpu() for x, _ in test_dataloader]\n",
    "logits = torch.cat(logits, dim=0)\n",
    "logits[:, seen_mask] *= (0.95 if DATASET != \"CUB\" else 1.0) # Trading a bit of gzsl-s for a bit of gzsl-u\n",
    "preds_gzsl = logits.argmax(dim=1).numpy()\n",
    "preds_zsl_s = logits[:, seen_mask].argmax(dim=1).numpy()\n",
    "preds_zsl_u = logits[:, ~seen_mask].argmax(dim=1).numpy()\n",
    "guessed_zsl_u = (preds_zsl_u == test_labels_remapped_unseen)\n",
    "guessed_gzsl = (preds_gzsl == test_labels)\n",
    "zsl_unseen_acc = np.mean([guessed_zsl_u[cls_idx].mean().item() for cls_idx in [class_indices_inside_test[c] for c in unseen_classes]]) \n",
    "gzsl_seen_acc = np.mean([guessed_gzsl[cls_idx].mean().item() for cls_idx in [class_indices_inside_test[c] for c in seen_classes]])\n",
    "gzsl_unseen_acc = np.mean([guessed_gzsl[cls_idx].mean().item() for cls_idx in [class_indices_inside_test[c] for c in unseen_classes]])\n",
    "gzsl_harmonic = 2 * (gzsl_seen_acc * gzsl_unseen_acc) / (gzsl_seen_acc + gzsl_unseen_acc)\n",
    "\n",
    "print(f'ZSL-U: {zsl_unseen_acc * 100:.02f}')\n",
    "# print(f'GZSL-U: {gzsl_unseen_acc * 100:.02f}')\n",
    "# print(f'GZSL-S: {gzsl_seen_acc * 100:.02f}')\n",
    "# print(f'GZSL-H: {gzsl_harmonic * 100:.02f}')\n",
    "\n",
    "# Best Result on hidden dim=1024, and 100 epochs acc= 32.76 and lr=0.0005 for vitL14 features\n",
    "# Best Result on hidden dim=1024, and 100 epochs acc= 17.09 and lr=0.0005 for vitG14 features\n",
    "# Best Result on hidden dim=512, and 100 epochs acc= 28.50 and lr=0.0005  for vitL14 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Different Normalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<=============== Loading data for AWA2 ===============>\n",
      "<=============== Preprocessing ===============>\n",
      "\n",
      "<=============== Starting training ===============>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/101 [00:00<?, ?it/s]/tmp/ipykernel_161780/2288364741.py:139: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  feats = torch.from_numpy(np.array(batch[0])).to(DEVICE)\n",
      "/tmp/ipykernel_161780/2288364741.py:140: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  targets = torch.from_numpy(np.array(batch[1])).to(DEVICE)\n",
      "100%|██████████| 101/101 [01:18<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done! Took time:  78.2 seconds\n",
      "Epoch 100 Results\n",
      "ZSL-U: 31.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DATASET = 'AWA2' # One of [\"AWA1\", \"AWA2\", \"APY\", \"CUB\", \"SUN\"]\n",
    "USE_CLASS_STANDARTIZATION = True # i.e. equation (9) from the paper\n",
    "USE_PROPER_INIT = True # i.e. equation (10) from the paper\n",
    "\n",
    "import numpy as np; np.random.seed(1)\n",
    "import torch; torch.manual_seed(1)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from scipy import io\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "print(f'<=============== Loading data for {DATASET} ===============>')\n",
    "DEVICE = 'cuda:1' # Set to 'cpu' if a GPU is not available\n",
    "DATA_DIR = f'./IAB-GZSL/data/{DATASET}'\n",
    "data = io.loadmat(f'{DATA_DIR}/vitL14.mat')\n",
    "attrs_mat = io.loadmat(f'{DATA_DIR}/att_splits.mat')\n",
    "feats = data['features'].T.astype(np.float32)\n",
    "labels = data['labels'].squeeze() - 1 # Using \"-1\" here and for idx to normalize to 0-index\n",
    "train_idx = attrs_mat['trainval_loc'].squeeze() - 1\n",
    "test_seen_idx = attrs_mat['test_seen_loc'].squeeze() - 1\n",
    "test_unseen_idx = attrs_mat['test_unseen_loc'].squeeze() - 1\n",
    "test_idx = np.array(test_seen_idx.tolist() + test_unseen_idx.tolist())\n",
    "seen_classes = sorted(np.unique(labels[test_seen_idx]))\n",
    "unseen_classes = sorted(np.unique(labels[test_unseen_idx]))\n",
    "\n",
    "\n",
    "print(f'<=============== Preprocessing ===============>')\n",
    "num_classes = len(seen_classes) + len(unseen_classes)\n",
    "seen_mask = np.array([(c in seen_classes) for c in range(num_classes)])\n",
    "unseen_mask = np.array([(c in unseen_classes) for c in range(num_classes)])\n",
    "attrs = attrs_mat['att'].T\n",
    "attrs = torch.from_numpy(attrs).to(DEVICE).float()\n",
    "attrs = attrs / attrs.norm(dim=1, keepdim=True) * np.sqrt(attrs.shape[1])\n",
    "attrs_seen = attrs[seen_mask]\n",
    "attrs_unseen = attrs[unseen_mask]\n",
    "train_labels = labels[train_idx]\n",
    "test_labels = labels[test_idx]\n",
    "test_seen_idx = [i for i, y in enumerate(test_labels) if y in seen_classes]\n",
    "test_unseen_idx = [i for i, y in enumerate(test_labels) if y in unseen_classes]\n",
    "labels_remapped_to_seen = [(seen_classes.index(t) if t in seen_classes else -1) for t in labels]\n",
    "test_labels_remapped_seen = [(seen_classes.index(t) if t in seen_classes else -1) for t in test_labels]\n",
    "test_labels_remapped_unseen = [(unseen_classes.index(t) if t in unseen_classes else -1) for t in test_labels]\n",
    "ds_train = [(feats[i], labels_remapped_to_seen[i]) for i in train_idx]\n",
    "ds_test = [(feats[i], int(labels[i])) for i in test_idx]\n",
    "train_dataloader = DataLoader(ds_train, batch_size=256, shuffle=True)\n",
    "test_dataloader = DataLoader(ds_test, batch_size=2048)\n",
    "\n",
    "class_indices_inside_test = {c: [i for i in range(len(test_idx)) if labels[test_idx[i]] == c] for c in range(num_classes)}\n",
    "\n",
    "\n",
    "class ClassStandardization(nn.Module):\n",
    "    \"\"\"\n",
    "    Class Standardization procedure from the paper.\n",
    "    Conceptually, it is equivalent to nn.BatchNorm1d with affine=False,\n",
    "    but for some reason nn.BatchNorm1d performs slightly worse.\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.running_mean = nn.Parameter(torch.zeros(feat_dim), requires_grad=False)\n",
    "        self.running_var = nn.Parameter(torch.ones(feat_dim), requires_grad=False)\n",
    "    \n",
    "    def forward(self, class_feats):\n",
    "        \"\"\"\n",
    "        Input: class_feats of shape [num_classes, feat_dim]\n",
    "        Output: class_feats (standardized) of shape [num_classes, feat_dim]\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            batch_mean = class_feats.mean(dim=0)\n",
    "            batch_var = class_feats.var(dim=0)\n",
    "            \n",
    "            # Normalizing the batch\n",
    "            result = (class_feats - batch_mean.unsqueeze(0)) / (batch_var.unsqueeze(0) + 1e-5)\n",
    "            \n",
    "            # Updating the running mean/std\n",
    "            self.running_mean.data = 0.9 * self.running_mean.data + 0.1 * batch_mean.detach()\n",
    "            self.running_var.data = 0.9 * self.running_var.data + 0.1 * batch_var.detach()\n",
    "        else:\n",
    "            # Using accumulated statistics\n",
    "            # Attention! For the test inference, we cant use batch-wise statistics,\n",
    "            # only the accumulated ones. Otherwise, it will be quite transductive\n",
    "            result = (class_feats - self.running_mean.unsqueeze(0)) / (self.running_var.unsqueeze(0) + 1e-5)\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "class CNZSLModel(nn.Module):\n",
    "    def __init__(self, attr_dim: int, hid_dim: int, proto_dim: int):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(attr_dim, hid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.GroupNorm(8, hid_dim),\n",
    "            \n",
    "            nn.Linear(hid_dim, hid_dim),\n",
    "            ClassStandardization(hid_dim) if USE_CLASS_STANDARTIZATION else nn.Identity(),\n",
    "            nn.ReLU(),\n",
    "            nn.GroupNorm(8, hid_dim),\n",
    "            \n",
    "            ClassStandardization(hid_dim) if USE_CLASS_STANDARTIZATION else nn.Identity(),\n",
    "            nn.Linear(hid_dim, proto_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        if USE_PROPER_INIT:\n",
    "            weight_var = 1 / (hid_dim * proto_dim)\n",
    "            b = np.sqrt(3 * weight_var)\n",
    "            self.model[-2].weight.data.uniform_(-b, b)\n",
    "        # Apply spectral normalization\n",
    "        self.model[0] = torch.nn.utils.spectral_norm(self.model[0])\n",
    "        self.model[3] = torch.nn.utils.spectral_norm(self.model[3])\n",
    "        \n",
    "    def forward(self, x, attrs):\n",
    "        protos = self.model(attrs)\n",
    "        protos = nn.LayerNorm(self.hid_dim).to(DEVICE)(protos)\n",
    "        x = nn.InstanceNorm1d(1, affine=False).to(DEVICE)(x.unsqueeze(1)).squeeze(1)\n",
    "        x_ns = 5 * x / x.norm(dim=1, keepdim=True) # [batch_size, x_dim]\n",
    "        protos_ns = 5 * protos / protos.norm(dim=1, keepdim=True) # [num_classes, x_dim]\n",
    "        logits = x_ns @ protos_ns.t() # [batch_size, num_classes]\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "\n",
    "print(f'\\n<=============== Starting training ===============>')\n",
    "start_time = time()\n",
    "model = CNZSLModel(attrs.shape[1],1024, feats.shape[1]).to(DEVICE)\n",
    "optim = torch.optim.AdamW(model.model.parameters(), lr=0.0005, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optim, gamma=0.1, step_size=25)\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(101)):\n",
    "    model.train()\n",
    "    \n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        feats = torch.from_numpy(np.array(batch[0])).to(DEVICE)\n",
    "        targets = torch.from_numpy(np.array(batch[1])).to(DEVICE)\n",
    "        logits = model(feats, attrs[seen_mask])\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "print(f'Training is done! Took time: {(time() - start_time): .1f} seconds')\n",
    "print(f\"Epoch {epoch} Results\")\n",
    "model.eval() # Important! Otherwise we would use unseen batch statistics\n",
    "logits = [model(x.to(DEVICE), attrs).cpu() for x, _ in test_dataloader]\n",
    "logits = torch.cat(logits, dim=0)\n",
    "logits[:, seen_mask] *= (0.95 if DATASET != \"CUB\" else 1.0) # Trading a bit of gzsl-s for a bit of gzsl-u\n",
    "preds_gzsl = logits.argmax(dim=1).numpy()\n",
    "preds_zsl_s = logits[:, seen_mask].argmax(dim=1).numpy()\n",
    "preds_zsl_u = logits[:, ~seen_mask].argmax(dim=1).numpy()\n",
    "guessed_zsl_u = (preds_zsl_u == test_labels_remapped_unseen)\n",
    "guessed_gzsl = (preds_gzsl == test_labels)\n",
    "zsl_unseen_acc = np.mean([guessed_zsl_u[cls_idx].mean().item() for cls_idx in [class_indices_inside_test[c] for c in unseen_classes]]) \n",
    "gzsl_seen_acc = np.mean([guessed_gzsl[cls_idx].mean().item() for cls_idx in [class_indices_inside_test[c] for c in seen_classes]])\n",
    "gzsl_unseen_acc = np.mean([guessed_gzsl[cls_idx].mean().item() for cls_idx in [class_indices_inside_test[c] for c in unseen_classes]])\n",
    "gzsl_harmonic = 2 * (gzsl_seen_acc * gzsl_unseen_acc) / (gzsl_seen_acc + gzsl_unseen_acc)\n",
    "print(f'ZSL-U: {zsl_unseen_acc * 100:.02f}')\n",
    "\n",
    "\n",
    "# Best Result on hidden dim=1024, and 100 epochs acc= 32.76 and lr=0.0005 for vitL14 features\n",
    "# Best Result on hidden dim=1024, and 100 epochs acc= 17.09 and lr=0.0005 for vitG14 features\n",
    "# Best Result on hidden dim=512, and 100 epochs acc= 28.50 and lr=0.0005  for vitL14 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Class_Normaliation_for_Continual_Zero_Shot_Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dgcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
